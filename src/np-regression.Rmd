---
title: "Non-parametric regressions"
output:
  html_document: default
---

Intro to non-parametric regressions.

```{r}
require(ggplot2)
require(data.table)
require(reshape2)
X = runif(1000);
Y = 10*((X-0.5)^3 -  0.1*X) + 0.2*rnorm(1000)
data = data.table(x=X,y=Y,yt=10*((X-0.5)^3 -  0.1*X))

ggplot(data,aes(x=x,y=y)) + geom_point() + geom_line(aes(y=yt),color="red",size=2) +theme_bw()
```

```{r}
fit = lm(y~x,data)
data = data[,y_hat := predict(fit)]

ggplot(data,aes(x=x,y=y)) + geom_point() + 
  geom_line(aes(y=yt),color="red",size=2) + geom_line(aes(y=y_hat),color="blue",size=2) +theme_bw()
```


# A simple tree regression

```{r}
require(rpart)
fit = rpart(y~x,method="anova",data)
data = data[,y_hat_tree := predict(fit)]

ggplot(data,aes(x=x,y=y)) + geom_point() + geom_line(aes(y=yt),color="red",size=2) + 
  geom_line(aes(y=y_hat_tree),color="green",size=2)+theme_bw()

```

```{r}
# plot tree
plot(fit, uniform=TRUE, main="Classification Tree for Chemicals")
text(fit, use.n=TRUE, all=TRUE, cex=.8)
```

# Kernel Estimator

```{r}
# show one kernel

g <- function(x0,h) data[, sum( y * dnorm( (x-x0)/h))/sum( dnorm( (x-x0)/h)) ]
data = data[, kern_val1 := dnorm( (x-0.25)/0.1) ,x]
data = data[, kern_val2 := dnorm( (x-0.25)/0.01) ,x]

ggplot(data,aes(x=x,y=kern_val1)) + geom_line() + geom_point(aes(y=y,alpha=kern_val1)) + theme_bw() 
ggplot(data,aes(x=x,y=kern_val2)) + geom_line() + geom_point(aes(y=y,alpha=kern_val2)) + theme_bw()

```



```{r}

g <- function(x0,h) data[, sum( y * dnorm( (x-x0)/h))/sum( dnorm( (x-x0)/h)) ]

data = data[, y_hat_kern := g(x,0.001),x]
ggplot(data,aes(x=x,y=y)) + geom_point() + geom_line(aes(y=y_hat_kern),color="green",size=1) + geom_line(aes(y=yt),color="red",size=2) + 
   theme_bw()

```

# doing cross-validation

```{r}

data = data[,i:=1:.N]

g2 <- function(x0,h,ii) data[i!=ii, sum( y * dnorm( (x-x0)/h))/sum( dnorm( (x-x0)/h)) ]

# for each point, we estimate the model leaving it out, we then predict the value for that point
# we then compute the distance with y

mse = rep(0,50)
hs = seq(0.002,0.05,l=50)
for (j in 1:50) {
  data[, y_hat_lo := g2(x,hs[j],i),i]
  mse[j] = data[,sum((y-y_hat_lo)^2)]
}

hstar = hs[which.min(mse)] 
ggplot(data.frame(hs,mse),aes(x=hs,y=mse)) + geom_point() + geom_line() + theme_bw() + geom_vline(xintercept = hstar,color="red",linetype=2)

```

```{r}

g <- function(x0,h) data[, sum( y * dnorm( (x-x0)/h))/sum( dnorm( (x-x0)/h)) ]
data = data[, y_hat_kern := g(x,hstar),x]

ggplot(data,aes(x=x,y=y)) + geom_point(alpha=0.5)  + 
  geom_line(aes(y=yt),color="red",size=1,linetype=2) + geom_line(aes(y=y_hat_kern),color="blue",size=1) +  theme_bw() 



```

# Sieve estimator

Here we use a simple basis function approach.

```{r}
require(splines)
n = 10
D = spline.des(seq(-0.2,1.2,l=n),x = data$x,outer.ok=TRUE)

rr = data.frame(D$design)
rr$x = data$x
rr = melt(rr,id.vars = c("x"))

ggplot(rr,aes(x=x,group=variable,y=value)) + geom_line() + theme_bw()

```

```{r}
# fitting 

fit = lm(data$y ~ D$design)
data =  data[,y_hat_spline := predict(fit)]

ggplot(data,aes(x=x,y=y)) + geom_point(alpha=0.5)  + 
  geom_line(aes(y=yt),color="red",size=1,linetype=2) + geom_line(aes(y=y_hat_spline),color="blue",size=1) +  theme_bw() 



```





