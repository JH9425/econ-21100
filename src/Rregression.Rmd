---
title: "ECON 21130 - R regression"
author: "Wooyong Lee"
header-includes:
- \usepackage{amsfonts,amssymb,amsmath}
- \usepackage{graphicx}
- \usepackage{setspace}
- \usepackage{cleveref}
output:
  html_document:
    toc: true
    toc_depth: 2
    toc_float:
      collapsed: false
      smooth_scroll: false
---

\newcommand{\beq}{\begin{equation}}
\newcommand{\eeq}{\end{equation}}
\newcommand{\beqr}{\begin{eqnarray}}
\newcommand{\eeqr}{\end{eqnarray}}
\newcommand{\nn}{\nonumber\\}

\newcommand{\var}{\textrm{Var}}
\newcommand{\cov}{\textrm{Cov}}
\newcommand{\corr}{\textrm{Corr}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\pp}{\mathbb{P}}
\newcommand{\veps}{\varepsilon}
\newcommand{\diag}{\textrm{diag}}

\newcommand{\by}{\bar Y}
\newcommand{\bx}{\bar X}
\newcommand{\beps}{\bar\varepsilon}
\newcommand{\pt}{\partial}

\newcommand{\tb}{\textbf}
\newcommand{\ti}{\textit}
\newcommand{\mb}{\mathbb}
\newcommand{\mc}{\mathcal}
\newcommand{\st}{\qquad\text{subject to}\qquad}
\newcommand{\ra}{\quad\Rightarrow\quad}

\newcommand{\bsni}{\bigskip\noindent}

```{r header, echo=FALSE, message=FALSE, warning=FALSE}
Sys.setenv(lang="EN")
```


# Linear regression

## The lm function

The function that does the regression is `lm`. To see how it works, let's use this dataset as an example.

```{r lm}
library(datasets)
head(mtcars) # this prints the first 6 observations of the dataset
```

If we want to regress `mpg` on a constant and `wt`, we write this.

```{r lm2}
regressionFit = lm(mpg ~ wt, data = mtcars)
```

We saved the return value of `lm` to `regressionFit`. It is a complicated object:

```{r lm3}
str(regressionFit)
```

But we see familiar names such as `coefficients`, `residuals`, and `fitted.values`. We can access these in the way that we access subvariables in a list.

```{r lm3-2}
regressionFit$coefficients
regressionFit$residuals
regressionFit$fitted.values
```



To see the usual results that we get from other languages, type these:

```{r lm4}
regressionFit
print(regressionFit)
summary(regressionFit)
```

To run a regression without a constant, do the following.

```{r lm5}
regFitWithoutConst = lm(mpg ~ -1 + wt, data=mtcars)
summary(regFitWithoutConst)
```

We can also add other regressors.

```{r lm6}
regressionFit = lm(mpg ~ wt + cyl + disp, data=mtcars)
summary(regressionFit)
```

As we have seen in the `ggplot2` tutorial, the variable `cyl` has only three values: `4`, `6,` `8`. We may want to treat `cyl` as a categorical variable and not a continuous variable. To do this so that we regress `mpg` on indicators of `cyl`, we use the `factor` function.

```{r lm7}
regressionFit = lm(mpg ~ wt + factor(cyl) + disp, data=mtcars)
summary(regressionFit)
```

If you want to use `wt^2` as a regressor, there are two ways to do it. One way is to create another column in the `data.frame`.

```{r lm8}
mtcars$wt2 = mtcars$wt^2 # the dataframe creates the column wt2 and assign the values.
head(mtcars)
summary(lm(mpg ~ wt + wt2, data=mtcars))
```

Another way that does not involve creating a column is the following.

```{r lm9}
summary(lm(mpg ~ wt + I(wt^2), data=mtcars))
```

The sums work similarly: e.g. `I(cyl+disp)`.

What is the function `I()`? The answer is related to what is the nature of the first argument of `lm`.

## The formula

We have been omitting the label for the first argument. The first argument of the `lm` function is `formula`:

```{r formula}
lm(formula = mpg ~ wt + disp, data=mtcars)
```

`formula` is a special object that interprets "expression". Note that we don't need to specify `mpg ~ wt + disp` as string, in which case we need to write `"mpg ~ wt + disp"`. In this "expression", the operators like `~` and `+` work differently from the usual way. For example, `+` in the formula is not an arithmetic operator, but an operator that says we have multiple regressors.

The function `I` orders R to read operators like `+` and `^` inside `I` as an arithmetic operator. So the operator `^` in `I(wt^2)` is interpreted as a power operator. Similarly, `I(cyl+disp)` interprets `+` as an arithmetic operator and not the operator that says we have two regressors `cyl` and `disp`.

## Heteroskedasticity

The `lm` function computes standard errors under the constant-variance assumption. To account for heteroskedasticity, we use the `sandwich` and the `lmtest` package. 

Let's first load the package.

```{r sandwich}
library(sandwich)
library(lmtest)
```

We use the following regression as an example.

```{r}
lfit = lm(formula = mpg ~ wt + disp, data=mtcars)
summary(lfit)
```

As mentioned earlier, the standard errors in the above regression are based on the constant-variance assumption. Now we compute the heteroskedasticity-robust standard error. We use the `vcovHC` function in the `sandwich` package:

```{r}
vcHC = vcovHC(lfit, type = "HC0")
```

Note that we use the result of `lm` function as the first argument of `vcovHC`. 

The above command computes the variance-covariance matrix by the estimator

$$
  vcHC = (X'X)^{-1}X'\Omega X(X'X)^{-1}
$$

where
$$
  \Omega = \text{diag}(u_1^2, \ldots, u_N^2)
$$
where $u_i$ is the residual for individual $i$.

We can instead specify `HC1`, `HC2`, `HC3`, etc. as an argument to `type`. These are all finite-sample corrections to the above estimator.

Now, to generate the analog of `summary(lfit)`, we use the `coeftest` function in the `lmtest` package:

```{r}
coeftest(lfit, vcov. = vcHC)
```

Compare the above result with the result of the `lm` function. We can see that the standard errors are corrected in the above. 

```{r}
summary(lfit)
```

## Clustered errors

We can also compute clustered standard errors using `sandwich` package. However, to do so, we need to tell `vcovHC` that what variables represent the group and the time indices. We need the `plm` package for this.

```{r}
library(plm)
```

The `plm` function in the `plm` package allows to do several kinds of panel data regressions. However, at this time, we use `plm` to do exactly what `lm` does, except that we store information on what are the group and the time indices. 

To discuss how to use `plm`, we use the following dataset as an example.
```{r}
data("Grunfeld", package = "plm")
head(Grunfeld)
```

Consider the following regression which uses `lm` function.

```{r}
pfitl = lm(inv ~ value + capital, data=Grunfeld)
summary(pfitl)
```

The following code produces exactly the same result:

```{r}
pfit = plm(inv ~ value + capital, model="pooling", index="firm", data=Grunfeld)
summary(pfit)
```

The option `model="pooling"` makes `plm` to run the usual regression that `lm` does. The option `index=firm` tells `plm` that the observations are grouped according to the `firm` variable.

Now we create the clustered standard errors by the following code.

```{r}
vcHCcluster = vcovHC(pfit, type = "HC0", cluster = "group")
```

Note that the value for the argument `cluster`, which is `"group"`, is not the name of the variable in the dataset `Grunfeld`. The option `cluster = "group"` tells that we use the group index saved in `pfit` as the cluster.

The above command computes the estimator

$$
  vcHCcluster = \left(\sum_{k=1}^K X_k'X_k\right)^{-1} \sum_{k=1}^K X_k'U_kU_k'X_k \left(\sum_{k=1}^K X_k'X_k\right)^{-1}
$$

where $X_k$ is the matrix of regressors for the $k$th cluster and $U_k$ is the residual vector for the $k$th cluster.

To correct for standard errors, we again use the `coeftest` function.

```{r}
coeftest(pfit, vcov. = vcHCcluster)
```

# Probit and logit

For probit and logit, we use `glm`. Using it is very much similar to using `lm`.

```{r glm}
# recall:
head(mtcars)

# let's run probit with some random formula. 
probitFit = glm(am ~ mpg + disp, family = binomial(link="probit"), data = mtcars)
probitFit
summary(probitFit)

# let's run logit.
logitFit = glm(am ~ mpg + disp, family = binomial(link="logit"), data = mtcars)
logitFit
summary(logitFit)

```









