---
title: "Linear model - selection and regularization"
output:
  html_document: default
---



```{r}
require(ggplot2)
require(data.table)
require(reshape2)
require(foreach)
require(MASS)
require(stringr)
library(glmnet)

# let's consider a simple model with p independent regressors
p=10
beta = rnorm(p)
n  = 20
p2 = 15

rr = data.table(foreach (i = 1:500,.combine=rbind) %do% {
  X = array(rnorm(p2*n),c(n,p2));
  Y = X %*% c(beta,rep(0,p2-p)) + 0.5*rnorm(n)
  fit = lm(Y~0+X)
  res = data.frame(value=coef(fit),value0=c(beta,rep(0,p2-p)))
  res$rep = i
  res$name = rownames(res)
  res
})

ggplot(rr,aes(x=name,y=value)) + 
  stat_summary(geom="point", fun.y=mean, color="red") +
  stat_summary(geom="ribbon", fun.data=mean_cl_normal, width=0.1, conf.int=0.95, fill="lightblue")

ggplot(data,aes(x=x,y=y)) + geom_point() + geom_line(aes(y=yt),color="red",size=2) +theme_bw()
```

# checking the bias and variance


```{r}
# let's consider a simple model with p independent regressors
p    = 10
beta = rnorm(p)
n    = 20
p2   = 18

# let's regularize it

rr = data.table( foreach (lambda = seq(0,2,l=20),.combine=rbind) %:% foreach (i = 1:2000,.combine=rbind) %do% {
  X = array(rnorm(p2*n),c(n,p2));
  Y = X %*% c(beta,rep(0,p2-p)) + 0.5*rnorm(n)
  fit = lm.ridge(Y~0+X,lambda=lambda)
  res = data.frame(value=coef(fit),value0=c(beta,rep(0,p2-p)))
  res$rep = i
  res$name = rownames(res)
  res$lambda = lambda
  res
})

rs = rr[,list(bias=mean(value-value0)^2,var=var(value-value0),mse=mean((value-value0)^2)),list(name,lambda)]
ggplot(rs[name=="X1"],aes(x=lambda,y=mse)) + geom_line() + theme_bw() + 
  geom_line(aes(y=bias),color="red") + geom_line(aes(y=var),color="blue")

ls = unique(rr$lambda)[c(1,5,10,15)]
ggplot(rr[name=="X1"][lambda %in% ls],aes(x=value, group=lambda,fill=lambda,color=lambda)) + 
  geom_density(alpha=0.3) + geom_vline(xintercept = beta[1],linetype=2) + theme_bw() + xlim(0,3)

```

```{r}
# looking at the results in this case
r2 = rs[name %in% paste("X",1:p,sep=""),mean(mse),lambda]
lambda_star = r2[,{I=which.min(V1);lambda[I]}]

X = array(rnorm(p2*n),c(n,p2));
Y = X %*% c(beta,rep(0,p2-p)) + 0.5*rnorm(n)
fit  = lm.ridge(Y~0+X,lambda=lambda_star)
fit2 = lm.ridge(Y~0+X,lambda=0)
X = array(rnorm(p2*n),c(n,p2));
Y = X %*% c(beta,rep(0,p2-p))
fit3 = lm.ridge(Y~0+X,lambda=0)

rr2 = rbind(
  data.frame(as.list(coef(fit)),name="ridge"),
  data.frame(as.list(coef(fit2)),name="ols"),
    data.frame(as.list(coef(fit3)),name="true"))
rr2$name = paste(rr2$name)

rr3 = melt(rr2,id.vars = "name")
rr3$var = as.integer(str_replace(rr3$variable,"X",""))
ggplot(rr3,aes(x=var,y=value,color=name)) + geom_point() + geom_line() + theme_bw()

# add the lasso regression
cvfit <- glmnet::cv.glmnet(X, Y)

rr2 = rbind(rr2,rr2[1,])
rr2[4,1:p2] =  as.matrix(coef(cvfit, s = "lambda.1se"))[2:(p2+1)]
rr2$name[4] <- "lasso"

rr3 = melt(rr2,id.vars = "name")
rr3$var = as.integer(str_replace(rr3$variable,"X",""))
ggplot(rr3,aes(x=var,y=value,color=name)) + geom_point() + geom_line() + theme_bw()


```



